# **Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey**

## **Policy Search**  

1. **PEPG:** [Parameter-exploring policy gradients](https://www.sciencedirect.com/science/article/abs/pii/S0893608009003220), Sehnke F et al, 2010, Neural Networks.  
2. **NES:** [Natural evolution strategies](https://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf), Wierstra D et al, 2014, The Journal of Machine Learning Research.  
3. **OpenAI-ES:** [Evolution strategies as a scalable alternative to reinforcement learning](https://arxiv.org/abs/1703.03864), Salimans T et al, 2017.  
4. **GA:** [Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning](https://arxiv.org/abs/1712.06567), Such F P et al, 2017.  
5. **NS-ES/NSR-ES/NSRA-ES:** [Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents](https://proceedings.neurips.cc/paper/2018/hash/b1301141feffabac455e1f90a7de2054-Abstract.html), Conti E et al, 2018, NeurIPS.  
6. **TRES:** [Trust region evolution strategies](https://ojs.aaai.org/index.php/AAAI/article/view/4345), Liu G et al, 2019, AAAI.  
7. **Guided ES:** [Guided evolutionary strategies: Augmenting random search with surrogate gradients](http://proceedings.mlr.press/v97/maheswaranathan19a.html), Maheswaranathan N et al, 2019, ICML.  
8. **PBT:** [Population based training of neural networks](https://arxiv.org/abs/1711.09846), Jaderberg M et al, 2017.  
9. **PB2:** [Provably efficient online hyperparameter optimization with population-based bandits](https://proceedings.neurips.cc/paper/2020/hash/c7af0926b294e47e52e46cfebe173f20-Abstract.html), Parker-Holder J et al, 2020, Advances in Neural Information Processing Systems.  
10. **SEARL:** [Sample-efficient automated deep reinforcement learning](https://arxiv.org/abs/2009.01555), Franke J K H et al, 2020.  
11. **DERL:** [Embodied intelligence via learning and evolution](https://www.nature.com/articles/s41467-021-25874-z), Gupta A et al, 2021, Nature communications.  

******
## **Experience-guided**

1. **ERQL:** [Bootstrapping $ q $-learning for robotics from neuro-evolution results](https://ieeexplore.ieee.org/abstract/document/7879193), Zimmer M et al, 2017, IEEE.  
2. **GRP-PG:** [Gep-pg: Decoupling exploration and exploitation in deep reinforcement learning algorithms](https://proceedings.mlr.press/v80/colas18a.html), Colas C et al, 2018, ICML.  
3. **ERL:** [Evolution-guided policy gradient in reinforcement learning](https://proceedings.neurips.cc/paper/2018/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html), Khadka S et al, 2018, NeurIPS.
4. **CEM-RL:** [CEM-RL: Combining evolutionary and gradient-based methods for policy search](https://arxiv.org/abs/1810.01222), Pourchot A et al, 2018.  
5. **CERL:** [Collaborative evolutionary reinforcement learning](https://proceedings.mlr.press/v97/khadka19a.html), Khadka S et al, 2019, ICML.  
6. **PDERL:** [Proximal distilled evolutionary reinforcement learning](https://ojs.aaai.org/index.php/AAAI/article/view/5728), Bodnar C et al, 2020, AAAI.  
7. **RIM:** [Recruitment-imitation mechanism for evolutionary reinforcement learning](https://www.sciencedirect.com/science/article/abs/pii/S0020025520311828), L체 S et al, 2021, Information Sciences.  
8. **ESAC:** [Maximum mutation reinforcement learning for scalable control](https://arxiv.org/abs/2007.13690), Suri K et al, 2020.  
9. **QD-RL:** [Qd-rl: Efficient mixing of quality and diversity in reinforcement learning](https://arxiv.org/abs/2006.08505), Cideron G et al, 2020.  
10. **SUPE-RL:** [Genetic soft updates for policy evolution in deep reinforcement learning](https://openreview.net/forum?id=TGFO0DbD_pk), Marchesini E et al, 2020, ICLR.  

******
## **Modules-embedded**

1. **PPO-CMA:** [PPO-CMA: Proximal policy optimization with covariance matrix adaptation](https://ieeexplore.ieee.org/abstract/document/9231618), H채m채l채inen P et al, 2020, IEEE.  
2. **EPG:** [Evolved policy gradients](https://proceedings.neurips.cc/paper/2018/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html), Houthooft R et al, 2018, NeurIPS.  
3. **CGP:** [Q-learning for continuous actions with cross-entropy guided policies](https://arxiv.org/abs/1903.10605), Simmons-Edler R et al, 2019.  
4. **GRAC:** [Grac: Self-guided and self-regularized actor-critic](https://proceedings.mlr.press/v164/shao22a.html), Shao L et al, 2022, CoRL.  